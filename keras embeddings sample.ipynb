{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sairam\n"
     ]
    }
   ],
   "source": [
    "print(\"sairam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocabulary size is 1000 <br>\n",
    "embedding shape is 64 cordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.embedding.Embedding at 0x285fe098160>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = Embedding(1000, 64)\n",
    "embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 20, 8)             80000     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 160)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 161       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Instantiate embedding layer\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 8, input_length=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.summary()\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the IMDB pre-processed(encoded) data set from keras data util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a list of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>num_words</b>\tinteger or None. Words are ranked by how often they occur (in the training set) and only the num_words most frequent words are kept. Any less frequent word will appear as oov_char value in the sequence data. If None, all words are kept. Defaults to None, so all words are kept.\n",
    "\n",
    "<br>\n",
    "<b>skip_top</b>\tskip the top N most frequently occurring words (which may not be informative). These words will appear as oov_char value in the dataset. Defaults to 0, so no words are skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 58s 3us/step\n",
      "Shape of train -> (25000,), (25000,)\n",
      "Shape of train -> (25000,), (25000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "\n",
    "max_features = 10000     \n",
    "max_length   = 30\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features, skip_top=25)\n",
    "print(f\"Shape of train -> {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Shape of train -> {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Train - first sentence 218\n",
      "Length of Train - Second sentence 189\n",
      "Length of Test - first sentence 68\n",
      "Length of Test - Second sentence 260\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of Train - first sentence {len(X_train[0])}\")\n",
    "print(f\"Length of Train - Second sentence {len(X_train[1])}\")\n",
    "\n",
    "print(f\"Length of Test - first sentence {len(X_test[0])}\")\n",
    "print(f\"Length of Test - Second sentence {len(X_test[1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238.71364"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean([len(rec) for rec in X_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avergae length of number of words in a sentence in 238"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update the max-length\n",
    "max_length   = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding has to be done to make the sentence length consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_train (25000, 100)\n",
      "shape of X_test (25000, 100)\n"
     ]
    }
   ],
   "source": [
    "X_train = keras.utils.pad_sequences(X_train, maxlen=max_length)\n",
    "X_test  = keras.utils.pad_sequences(X_test, maxlen=max_length)\n",
    "\n",
    "print(f\"shape of X_train {X_train.shape}\")\n",
    "print(f\"shape of X_test {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1415,   33,    2,    2,    2,  215,   28,   77,   52,    2,    2,\n",
       "        407,    2,   82,    2,    2,    2,  107,  117, 5952,    2,  256,\n",
       "          2,    2,    2, 3766,    2,  723,   36,   71,   43,  530,  476,\n",
       "         26,  400,  317,   46,    2,    2,    2, 1029,    2,  104,   88,\n",
       "          2,  381,    2,  297,   98,   32, 2071,   56,   26,  141,    2,\n",
       "        194, 7486,    2,    2,  226,    2,    2,  134,  476,   26,  480,\n",
       "          2,  144,   30, 5535,    2,   51,   36,   28,  224,   92,   25,\n",
       "        104,    2,  226,   65,    2,   38, 1334,   88,    2,    2,  283,\n",
       "          2,    2, 4472,  113,  103,   32,    2,    2, 5345,    2,  178,\n",
       "         32])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_23/kernel:0', 'dense_23/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_23/kernel:0', 'dense_23/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "621/782 [======================>.......] - ETA: 40s - loss: 0.6978 - accuracy: 0.5022"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=X_train.shape[0], activation='relu'))\n",
    "model.add(Embedding(input_dim=max_features, output_dim=64, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=8, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "#model.summary()\n",
    "\n",
    "model.compile(metrics=['accuracy'], loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), verbose=1, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference\n",
    "https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38f28c8d6dbcdd87c15bb676d57b84bc771d7ce270d0318b594bead37f5d3a7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
